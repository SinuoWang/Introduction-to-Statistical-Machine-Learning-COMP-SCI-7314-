{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c489a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7712c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "print(cp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbed42",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f272996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-1.64</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.65</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.44</td>\n",
       "      <td>-1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.40</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-1.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.30</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>1.31</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-1.54</td>\n",
       "      <td>-1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.44</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-1.42</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>-1.13</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8495</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.55</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>-2.96</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.93</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.78</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.25</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.19</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>1.44</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>1.58</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>2.97</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>-1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>1.79</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-1.96</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-1.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8500 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9    ...   191  \\\n",
       "0     0.0 -0.36 -0.91 -0.99 -0.57 -1.38 -1.54 -1.64  1.29  0.65  ...  1.15   \n",
       "1     1.0 -1.40 -1.90  0.09  0.29 -0.30 -1.30  1.13 -2.38 -1.16  ...  0.48   \n",
       "2     1.0 -0.43  1.45 -0.68 -1.58  0.32 -0.14  0.23 -1.01 -0.39  ... -0.94   \n",
       "3     1.0 -0.76  0.30 -0.57 -0.33 -1.50  1.84  1.37  0.23  0.66  ... -0.42   \n",
       "4     0.0 -0.76  1.36  0.00 -1.44 -1.27 -0.76 -1.42 -0.58  0.11  ... -0.44   \n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "8495  1.0 -1.55 -0.02  0.67  0.17 -2.17 -2.96 -0.52  0.58 -0.26  ...  1.09   \n",
       "8496  0.0  0.53 -0.74  0.06 -0.76  0.20  0.02  0.75  1.53  0.34  ... -0.95   \n",
       "8497  1.0  1.18  0.78 -0.02  1.25 -0.85 -0.50  1.19 -0.73  0.33  ...  1.44   \n",
       "8498  1.0 -0.55  1.16 -0.56  1.58 -0.06 -1.38  0.94 -0.14  0.47  ...  0.11   \n",
       "8499  0.0  0.80 -0.97 -0.15 -0.01 -1.57  1.02  0.34 -0.39 -0.49  ...  1.79   \n",
       "\n",
       "       192   193   194   195   196   197   198   199   200  \n",
       "0    -0.05 -0.09  0.02  1.75  1.58  0.12  0.30  2.44 -1.26  \n",
       "1     0.24 -0.16 -0.48 -0.02 -0.35 -0.27 -0.20 -0.92 -0.46  \n",
       "2     0.11 -1.30 -0.24  0.74  0.88  1.37  0.12  0.01 -0.56  \n",
       "3     0.06 -1.05  0.35 -0.24 -0.69  1.31 -0.18 -1.54 -1.70  \n",
       "4     1.45 -1.18 -1.13 -0.14  0.04  0.33  1.20 -0.81 -1.16  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "8495  1.37  0.44 -0.97 -0.65  0.16 -2.17  0.39 -2.48  0.36  \n",
       "8496  0.97  0.25 -0.93 -0.60 -0.64  0.03 -1.04  0.01 -1.04  \n",
       "8497 -1.57 -0.08  0.55  1.06  0.36 -0.40 -0.63  0.29 -0.96  \n",
       "8498 -0.42 -0.81 -0.04 -0.11  0.42 -0.30  2.97 -2.66 -1.90  \n",
       "8499 -1.32  0.20 -1.96 -0.32  0.35  0.44 -1.58  0.35  1.06  \n",
       "\n",
       "[8500 rows x 201 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train.shape:  (4000, 200)\n",
      "labels_train.shape:  (4000,)\n",
      "features_val.shape:  (4499, 200)\n",
      "labels_val.shape:  (4499,)\n",
      "features_test.shape:  (1500, 200)\n",
      "labels_test.shape:  (1500,)\n"
     ]
    }
   ],
   "source": [
    "train_val_df = pd.read_csv('train.csv', header = None)\n",
    "display(train_val_df)\n",
    "train = train_val_df[:4000].to_numpy()\n",
    "X_train = train[:,1:]\n",
    "Y_train = train[:,0]\n",
    "Y_train[np.where(Y_train == 0)] = -1 # negative label 0 to -1\n",
    "print('features_train.shape: ',X_train.shape)\n",
    "print('labels_train.shape: ',Y_train.shape)\n",
    "\n",
    "validation = train_val_df[4001:].to_numpy()\n",
    "X_val = validation[:,1:]\n",
    "Y_val = validation[:,0]\n",
    "Y_val[np.where(Y_val == 0)] = -1 # negative label 0 to -1\n",
    "print('features_val.shape: ',X_val.shape)\n",
    "print('labels_val.shape: ',Y_val.shape)\n",
    "\n",
    "test_df = pd.read_csv('test.csv',header = None)\n",
    "test = test_df.to_numpy()\n",
    "X_test = test[:,1:]\n",
    "Y_test = test[:,0]\n",
    "Y_test[np.where(Y_test == 0)] = -1 # negative label 0 to -1\n",
    "print('features_test.shape: ',X_test.shape)\n",
    "print('labels_test.shape: ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5049c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.36 -0.91 -0.99 ...  0.3   2.44 -1.26]\n",
      " [-1.4  -1.9   0.09 ... -0.2  -0.92 -0.46]\n",
      " [-0.43  1.45 -0.68 ...  0.12  0.01 -0.56]\n",
      " ...\n",
      " [-0.57  0.14 -0.62 ...  0.    0.38 -0.82]\n",
      " [ 0.4   0.16 -0.49 ...  0.89  0.21  1.09]\n",
      " [ 0.86 -0.23 -2.01 ...  0.21  0.68  2.49]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train) #just double check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e569346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36 -0.91 -0.99 -0.57 -1.38 -1.54 -1.64  1.29  0.65 -0.75  1.11 -0.04\n",
      "  1.86  0.76 -0.67  0.75  0.07  0.31  1.23 -0.7  -0.31 -1.48  0.11 -1.18\n",
      "  0.78  0.48  0.85  2.11 -1.45  0.74 -0.54  1.31  0.64 -1.42  1.22 -1.35\n",
      "  0.4  -0.72 -0.79  0.8   0.32 -0.76  0.22  2.59 -0.64  1.27  0.66  1.8\n",
      "  0.77  1.13 -1.69 -1.31  1.66 -0.26 -0.17 -0.41 -0.8   1.24 -1.51 -1.45\n",
      " -1.07 -1.38 -0.92 -1.26 -2.02  0.58 -0.58 -1.06 -0.05  0.29 -1.72 -0.26\n",
      " -1.67 -0.07  0.81  0.01 -0.22  1.72  0.43  0.92 -0.68 -0.98 -1.12 -0.81\n",
      " -0.79  1.72 -1.33 -1.08 -1.37 -1.75 -0.9  -0.76 -1.33  0.37  0.25  0.24\n",
      "  0.33 -0.72 -1.05 -0.47 -0.44 -0.89  0.01 -1.29 -1.79 -0.39 -1.03  0.29\n",
      "  1.2   0.62 -0.19 -1.3  -0.29  2.5   2.01  0.75 -0.53  1.61 -1.62 -0.33\n",
      " -0.93 -0.79  0.89 -0.64  0.13  0.97  0.37  0.22  0.97 -0.29  1.5  -0.42\n",
      "  0.62  1.43  0.55 -1.23 -1.83  2.13 -0.06  0.76  0.63  0.58  1.92  0.03\n",
      "  0.69  0.6   1.18  0.54  0.03 -2.2   1.85 -0.78  2.04 -0.82 -0.14 -1.3\n",
      "  0.84  0.63 -0.44 -1.79  0.16  1.34  0.57 -1.01  1.19  0.43 -0.42  0.05\n",
      " -1.18  1.22  0.19  0.69 -2.78 -0.53 -1.16 -2.29  0.45 -0.12  0.84 -0.6\n",
      "  1.54 -0.32 -0.4  -0.45  1.21 -0.7   0.81  0.81  1.21 -2.44  1.15 -0.05\n",
      " -0.09  0.02  1.75  1.58  0.12  0.3   2.44 -1.26]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633c6e4",
   "metadata": {},
   "source": [
    "# Primal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55fc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm_model:\n",
    "    def __init__(self,W,b,loss = None,A = None):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.Psi = loss\n",
    "        self.Alpha = A\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95539604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict_primal(data_test, label_test, svm_model):\n",
    "    W = svm_model.W\n",
    "    b = svm_model.b\n",
    "    X = data_test\n",
    "    y = label_test\n",
    "    \n",
    "    total = data_test.shape[0]\n",
    "    \n",
    "    res = np.multiply(X @ W + b, y)\n",
    "    correct = len(np.where(res > 1e-4)[0]) #label*res to check whether they are the same sign\n",
    "    acc = correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "955d9784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:  optimal\n",
      "The sum of W*:  -0.1452156803361282\n",
      "b*:             1.779813717087077 \n",
      "\n",
      "Accuracy on the test set: 96.8%\n"
     ]
    }
   ],
   "source": [
    "def svm_train_primal(data_train, label_train, regularisation_para_C):\n",
    "    X_train = data_train\n",
    "    Y_train = label_train\n",
    "    \n",
    "    m,n = X_train.shape #m: number of samples, n: number of features\n",
    "    W = cp.Variable(n) # w: n*1\n",
    "    Psi = cp.Variable(m) # Psi: m*1, hinge loss, slack variable\n",
    "    b = cp.Variable() # b is a scalar parameter\n",
    "    C = regularisation_para_C # Hyper-parameter, control softness trade-off of the margin\n",
    "\n",
    "    # Objective in primal problem: \n",
    "    objective = cp.Minimize(0.5 * cp.sum_squares(cp.norm(W)) + (C/m) * cp.sum(Psi)) ####c/n\n",
    "\n",
    "    # Constraints in primal problem:\n",
    "    constraints = [cp.multiply(Y_train,X_train @ W + b) -1 + Psi >= 0, Psi >= 0]\n",
    "\n",
    "    # Solve constrained convex optimization\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    \n",
    "    # Show results\n",
    "    print('Status: ', prob.status)\n",
    "    #print('W*: ', W.value)\n",
    "    print('The sum of W*: ', np.sum(W.value))\n",
    "    print('b*:            ', b.value,'\\n')\n",
    "    \n",
    "    # Create svm prime model:\n",
    "    prime_model = svm_model(W.value,b.value,loss = Psi.value)\n",
    "    return prime_model\n",
    "\n",
    "\n",
    "# The I/O format requested in the assignment description\n",
    "prime_model = svm_train_primal(X_train, Y_train, 100)\n",
    "test_accuracy = svm_predict_primal(X_test,Y_test,prime_model)\n",
    "print(f'Accuracy on the test set: {test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c05c7a",
   "metadata": {},
   "source": [
    "# Dual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9ade578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:  optimal\n",
      "The sum of Alpha*:  7.28163240568506\n",
      "The sum of dual W*:  -0.1452157032864848\n",
      "Dual b*:             1.7942996486309701 \n",
      "\n",
      "Accuracy on the test set: 96.8%\n"
     ]
    }
   ],
   "source": [
    "def svm_train_dual(data_train, label_train, regularisation_para_C):\n",
    "    X_train = data_train\n",
    "    Y_train = label_train\n",
    "    C = regularisation_para_C\n",
    "    \n",
    "    m,n = X_train.shape #m: number of samples, n: number of features\n",
    "    Alpha = cp.Variable(m) # Alpha: size m, Lagrange multiplier\n",
    "    \n",
    "    # Objective in dual problem:\n",
    "    dual_objective = cp.Maximize(cp.sum(Alpha) - 0.5*cp.sum_squares(cp.multiply(Alpha,Y_train) @ X_train))\n",
    "    \n",
    "    \n",
    "    #Constraints in dual problem:\n",
    "    dual_constraints = [Alpha >= 0, Alpha <= (C/m), Alpha.T @ Y_train == 0] ##c/sample num\n",
    "    \n",
    "    # Solve constrained convex optimization\n",
    "    dual_prob = cp.Problem(dual_objective, dual_constraints)\n",
    "    # dual_prob.solve(verbose=True)\n",
    "    \n",
    "    #solvers will fail when the numerical data is very large or very small, which can lead to what's known as poorly conditioned problem data\n",
    "    dual_prob.solve(solver=cp.ECOS) #using the ECOS solver solved the OSQP Error\n",
    "\n",
    "\n",
    "    # Show results\n",
    "    print('Status: ', dual_prob.status)\n",
    "    print('The sum of Alpha*: ', np.sum(Alpha.value))\n",
    "\n",
    "    # Calculate W* and b* from dual problem\n",
    "    dual_W = X_train.T @ np.multiply(Alpha.value,Y_train)\n",
    "    dual_b = np.mean(Y_train - X_train @ dual_W)\n",
    "    print('The sum of dual W*: ', np.sum(dual_W))\n",
    "    print('Dual b*:            ', dual_b,'\\n')\n",
    "\n",
    "    \n",
    "    # Create svm dual model:\n",
    "    dual_model = svm_model(dual_W,dual_b,loss = None,A = Alpha.value)\n",
    "    return dual_model\n",
    "\n",
    "\n",
    "\n",
    "# The I/O format requested in the assignment description\n",
    "dual_model = svm_train_dual(X_train, Y_train, 100)\n",
    "\n",
    "# The 'svm_predict_primal()' below works for dual derived W and b as well. \n",
    "#The name of this function is fixed in the assignment requirement,\n",
    "#so I couldn't change it, even though it's an ambiguious name\n",
    "test_accuracy = svm_predict_primal(X_test,Y_test,dual_model) \n",
    "\n",
    "print(f'Accuracy on the test set: {test_accuracy*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb04bd",
   "metadata": {},
   "source": [
    "# Support Vectors Primal Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf3593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 392 support vectors:\n",
      "\n",
      "[[-0.36 -0.91 -0.99 ...  0.3   2.44 -1.26]\n",
      " [ 1.05 -1.79  0.9  ...  0.39  0.6  -1.66]\n",
      " [ 1.01 -1.13  1.49 ...  0.23 -0.3  -0.01]\n",
      " ...\n",
      " [ 2.16 -0.78 -0.78 ... -0.38  1.1   0.39]\n",
      " [ 0.36 -0.19 -1.06 ... -0.83 -0.2   0.12]\n",
      " [-0.73 -1.19 -0.24 ...  1.46 -1.36  1.21]] \n",
      "\n",
      "The index of support vectors in prime: \n",
      " [0, 16, 28, 29, 41, 54, 58, 68, 80, 88, 127, 130, 140, 145, 147, 150, 199, 208, 239, 253, 263, 266, 275, 281, 284, 296, 305, 307, 332, 344, 356, 377, 395, 401, 403, 405, 414, 420, 424, 432, 433, 441, 446, 450, 473, 479, 495, 510, 521, 525, 532, 547, 561, 563, 564, 567, 576, 581, 587, 595, 604, 610, 637, 642, 656, 660, 678, 703, 708, 709, 725, 736, 737, 752, 768, 774, 796, 799, 809, 816, 821, 826, 834, 843, 844, 856, 861, 873, 905, 909, 932, 935, 948, 965, 967, 988, 989, 993, 1003, 1017, 1022, 1050, 1082, 1098, 1116, 1123, 1139, 1142, 1166, 1167, 1178, 1179, 1188, 1219, 1242, 1266, 1274, 1295, 1304, 1320, 1364, 1371, 1374, 1377, 1403, 1406, 1407, 1414, 1416, 1437, 1447, 1474, 1475, 1478, 1489, 1492, 1493, 1531, 1532, 1536, 1539, 1555, 1571, 1592, 1594, 1598, 1604, 1616, 1627, 1635, 1636, 1651, 1659, 1664, 1667, 1672, 1675, 1680, 1683, 1684, 1691, 1692, 1705, 1706, 1722, 1724, 1737, 1775, 1778, 1780, 1799, 1825, 1833, 1835, 1840, 1848, 1851, 1852, 1854, 1859, 1862, 1864, 1865, 1869, 1884, 1886, 1889, 1890, 1900, 1921, 1924, 1957, 1964, 1972, 1982, 1993, 2012, 2021, 2032, 2035, 2039, 2047, 2049, 2051, 2064, 2066, 2070, 2088, 2099, 2100, 2108, 2112, 2132, 2136, 2145, 2167, 2186, 2189, 2215, 2249, 2257, 2263, 2269, 2278, 2281, 2283, 2288, 2289, 2306, 2309, 2316, 2352, 2356, 2360, 2361, 2374, 2390, 2406, 2407, 2416, 2436, 2438, 2446, 2452, 2458, 2482, 2490, 2497, 2513, 2518, 2544, 2546, 2550, 2558, 2570, 2575, 2620, 2622, 2624, 2642, 2645, 2647, 2648, 2658, 2688, 2691, 2699, 2703, 2705, 2711, 2715, 2718, 2725, 2728, 2734, 2751, 2770, 2813, 2817, 2857, 2861, 2874, 2876, 2885, 2902, 2905, 2910, 2919, 2925, 2937, 2945, 2953, 2964, 2978, 2998, 3003, 3014, 3018, 3052, 3056, 3066, 3072, 3079, 3082, 3083, 3099, 3111, 3127, 3145, 3159, 3165, 3180, 3188, 3198, 3203, 3205, 3212, 3216, 3232, 3244, 3248, 3284, 3297, 3326, 3362, 3364, 3369, 3371, 3372, 3376, 3389, 3397, 3404, 3414, 3423, 3429, 3456, 3462, 3465, 3467, 3472, 3481, 3483, 3485, 3510, 3517, 3519, 3533, 3535, 3558, 3562, 3564, 3575, 3588, 3589, 3596, 3599, 3607, 3611, 3613, 3649, 3651, 3659, 3672, 3684, 3687, 3712, 3725, 3734, 3735, 3740, 3752, 3772, 3809, 3811, 3835, 3844, 3845, 3846, 3869, 3893, 3905, 3916, 3925, 3930, 3942, 3950, 3953, 3969, 3985, 3990, 3991]\n"
     ]
    }
   ],
   "source": [
    "# Soft-Margin Support Vectors must satisfy: yi(W^*T @ Xi + b) = 1 - Psi\n",
    "\n",
    "#get the values from prime model\n",
    "W = prime_model.W\n",
    "b = prime_model.b\n",
    "Psi = prime_model.Psi\n",
    "\n",
    "prime_sv_idx = []\n",
    "for i in range(X_train.shape[0]): #iterate through each sample\n",
    "    X_i = X_train[i]\n",
    "    y_i = Y_train[i]\n",
    "    loss_i = Psi[i]\n",
    "        \n",
    "    res = y_i * (X_i @ W + b) -1 + loss_i\n",
    "    # to solve the precision problem, changed '== 0'  to a range around +-1e-4\n",
    "    if res < 1e-4 and res > -1e-4:  \n",
    "        prime_sv_idx.append(i)\n",
    "\n",
    "prime_sv = X_train[prime_sv_idx]\n",
    "print(f'There are {len(prime_sv_idx)} support vectors:\\n')\n",
    "print(prime_sv,'\\n')\n",
    "print(f'The index of support vectors in prime: \\n {prime_sv_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de399c",
   "metadata": {},
   "source": [
    "# Support Vectors Dual Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da7d2df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 392 support vectors:\n",
      "\n",
      "[[-0.36 -0.91 -0.99 ...  0.3   2.44 -1.26]\n",
      " [ 1.05 -1.79  0.9  ...  0.39  0.6  -1.66]\n",
      " [ 1.01 -1.13  1.49 ...  0.23 -0.3  -0.01]\n",
      " ...\n",
      " [ 2.16 -0.78 -0.78 ... -0.38  1.1   0.39]\n",
      " [ 0.36 -0.19 -1.06 ... -0.83 -0.2   0.12]\n",
      " [-0.73 -1.19 -0.24 ...  1.46 -1.36  1.21]] \n",
      "\n",
      "The index of support vectors in dual: \n",
      " [0, 16, 28, 29, 41, 54, 58, 68, 80, 88, 127, 130, 140, 145, 147, 150, 199, 208, 239, 253, 263, 266, 275, 281, 284, 296, 305, 307, 332, 344, 356, 377, 395, 401, 403, 405, 414, 420, 424, 432, 433, 441, 446, 450, 473, 479, 495, 510, 521, 525, 532, 547, 561, 563, 564, 567, 576, 581, 587, 595, 604, 610, 637, 642, 656, 660, 678, 703, 708, 709, 725, 736, 737, 752, 768, 774, 796, 799, 809, 816, 821, 826, 834, 843, 844, 856, 861, 873, 905, 909, 932, 935, 948, 965, 967, 988, 989, 993, 1003, 1017, 1022, 1050, 1082, 1098, 1116, 1123, 1139, 1142, 1166, 1167, 1178, 1179, 1188, 1219, 1242, 1266, 1274, 1295, 1304, 1320, 1364, 1371, 1374, 1377, 1403, 1406, 1407, 1414, 1416, 1437, 1447, 1474, 1475, 1478, 1489, 1492, 1493, 1531, 1532, 1536, 1539, 1555, 1571, 1592, 1594, 1598, 1604, 1616, 1627, 1635, 1636, 1651, 1659, 1664, 1667, 1672, 1675, 1680, 1683, 1684, 1691, 1692, 1705, 1706, 1722, 1724, 1737, 1775, 1778, 1780, 1799, 1825, 1833, 1835, 1840, 1848, 1851, 1852, 1854, 1859, 1862, 1864, 1865, 1869, 1884, 1886, 1889, 1890, 1900, 1921, 1924, 1957, 1964, 1972, 1982, 1993, 2012, 2021, 2032, 2035, 2039, 2047, 2049, 2051, 2064, 2066, 2070, 2088, 2099, 2100, 2108, 2112, 2132, 2136, 2145, 2167, 2186, 2189, 2215, 2249, 2257, 2263, 2269, 2278, 2281, 2283, 2288, 2289, 2306, 2309, 2316, 2352, 2356, 2360, 2361, 2374, 2390, 2406, 2407, 2416, 2436, 2438, 2446, 2452, 2458, 2482, 2490, 2497, 2513, 2518, 2544, 2546, 2550, 2558, 2570, 2575, 2620, 2622, 2624, 2642, 2645, 2647, 2648, 2658, 2688, 2691, 2699, 2703, 2705, 2711, 2715, 2718, 2725, 2728, 2734, 2751, 2770, 2813, 2817, 2857, 2861, 2874, 2876, 2885, 2902, 2905, 2910, 2919, 2925, 2937, 2945, 2953, 2964, 2978, 2998, 3003, 3014, 3018, 3052, 3056, 3066, 3072, 3079, 3082, 3083, 3099, 3111, 3127, 3145, 3159, 3165, 3180, 3188, 3198, 3203, 3205, 3212, 3216, 3232, 3244, 3248, 3284, 3297, 3326, 3362, 3364, 3369, 3371, 3372, 3376, 3389, 3397, 3404, 3414, 3423, 3429, 3456, 3462, 3465, 3467, 3472, 3481, 3483, 3485, 3510, 3517, 3519, 3533, 3535, 3558, 3562, 3564, 3575, 3588, 3589, 3596, 3599, 3607, 3611, 3613, 3649, 3651, 3659, 3672, 3684, 3687, 3712, 3725, 3734, 3735, 3740, 3752, 3772, 3809, 3811, 3835, 3844, 3845, 3846, 3869, 3893, 3905, 3916, 3925, 3930, 3942, 3950, 3953, 3969, 3985, 3990, 3991]\n"
     ]
    }
   ],
   "source": [
    "# In dual problem, only the samples that associated with a positive Lagranian coefficient \n",
    "# (i.e. Alpha[i] > 0) can have effect on the decision boundary. These samples are the support vectors.\n",
    "\n",
    "Alpha = dual_model.Alpha\n",
    "\n",
    "dual_sv_idx = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    if Alpha[i] > 1e-4:\n",
    "        dual_sv_idx.append(i)\n",
    "\n",
    "dual_sv = X_train[dual_sv_idx]\n",
    "print(f'There are {len(dual_sv_idx)} support vectors:\\n')\n",
    "print(dual_sv,'\\n')\n",
    "print(f'The index of support vectors in dual: \\n {dual_sv_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3937889",
   "metadata": {},
   "source": [
    "# Q5. Hyper-parameter 'C' Tuning Utilizing Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dbb2c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\lib\\site-packages\\cvxpy\\problems\\problem.py:1337: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:  optimal_inaccurate\n",
      "The sum of W*:  0.00025768720168650424\n",
      "b*:             0.3975700168285553 \n",
      "\n",
      "C = 2^(-10) validation accuracy: 49.09979995554568%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  0.001018431458808717\n",
      "b*:             0.9491146973216334 \n",
      "\n",
      "C = 2^(-8) validation accuracy: 49.09979995554568%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  0.004074508427878487\n",
      "b*:             0.7964686784653918 \n",
      "\n",
      "C = 2^(-6) validation accuracy: 49.09979995554568%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  0.009485958765916944\n",
      "b*:             0.33673963217431 \n",
      "\n",
      "C = 2^(-4) validation accuracy: 92.44276505890198%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.01876808041378697\n",
      "b*:             0.50062747239116 \n",
      "\n",
      "C = 2^(-2) validation accuracy: 96.22138252945099%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.08009345876340557\n",
      "b*:             0.6779831450796824 \n",
      "\n",
      "C = 2^(0) validation accuracy: 97.17715047788398%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.09799142609364914\n",
      "b*:             0.9130704080137506 \n",
      "\n",
      "C = 2^(2) validation accuracy: 97.48833074016449%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.23126597418146339\n",
      "b*:             1.2761393758705935 \n",
      "\n",
      "C = 2^(4) validation accuracy: 97.39942209379862%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.06691532625610219\n",
      "b*:             1.6515083137107895 \n",
      "\n",
      "C = 2^(6) validation accuracy: 97.13269615470105%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.5697580787205915\n",
      "b*:             2.175596789589616 \n",
      "\n",
      "C = 2^(8) validation accuracy: 96.59924427650589%\n",
      "--------------------------------------------\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.5414191179747649\n",
      "b*:             2.573993935133974 \n",
      "\n",
      "C = 2^(10) validation accuracy: 96.26583685263392%\n",
      "--------------------------------------------\n",
      "Best C is  2^(2) , which  gives 97.48833074016449% accuracy.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set performance:\n",
      "Status:  optimal\n",
      "The sum of W*:  -0.09799142609364914\n",
      "b*:             0.9130704080137506 \n",
      "\n",
      "C = 2^(2) gives accuracy: 97.46666666666667% on the test set.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create hyper-parameter search space c = [2^-10, 2^-8, ...,2^8, 2^10]\n",
    "expo = np.linspace(-10,10,num = 11)\n",
    "C_lst = [2**(ex) for ex in expo]\n",
    "str_lst = ['2^('+str(int(ex))+')' for ex in expo]\n",
    "C_dict = dict(zip(str_lst,C_lst))\n",
    "\n",
    "#initializing\n",
    "best_score = 0\n",
    "best_C_idx = 0\n",
    "acc_lst = []\n",
    "#find best C on validation set\n",
    "for i,(C_str,C) in enumerate(C_dict.items()):\n",
    "    p_model = svm_train_primal(X_train, Y_train, C) #create svm model for new C\n",
    "    val_accuracy = svm_predict_primal(X_val,Y_val,p_model)\n",
    "    print(f'C = {C_str} validation accuracy: {val_accuracy*100}%')\n",
    "    acc_lst.append(val_accuracy)\n",
    "    if val_accuracy > best_score:\n",
    "        best_score = val_accuracy\n",
    "        best_C_idx = i #keep the best one\n",
    "    print('--------------------------------------------')\n",
    "        \n",
    "print('Best C is ',str_lst[best_C_idx], f', which  gives {best_score*100}% accuracy.\\n')\n",
    "plt.plot(C_lst,acc_lst)\n",
    "plt.xlabel('C values')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.title('C values vs Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Optimal C performance on the test set\n",
    "print('Test set performance:')\n",
    "test_model = svm_train_primal(X_train, Y_train, C_lst[best_C_idx])\n",
    "test_accuracy = svm_predict_primal(X_test,Y_test,test_model)\n",
    "print(f'C = {str_lst[best_C_idx]} gives accuracy: {test_accuracy*100}% on the test set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b9f3244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(C_lst,acc_lst)\n",
    "plt.xlabel('C values (log scale)')\n",
    "plt.ylabel('Validation accuracy')\n",
    "plt.xscale(\"log\")\n",
    "plt.title('C values vs Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1b76a",
   "metadata": {},
   "source": [
    "# Q6. Sklearn SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f483177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bea875fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.66666666666667%\n",
      "sum of W*:  -0.03860344484587469\n",
      "b*:  0.29306619973990966\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Using the best C in Q5\n",
    "clf = svm.LinearSVC(C = C_lst[best_C_idx]/X_train.shape[0])\n",
    "clf.fit(X_train, Y_train)\n",
    "score = clf.score(X_test,Y_test)\n",
    "print(f'Test accuracy: {score*100}%')\n",
    "print('sum of W*: ',np.sum(clf.coef_))\n",
    "print('b*:        ',clf.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df4b0b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.44140625e-07,\n",
       " 9.765625e-07,\n",
       " 3.90625e-06,\n",
       " 1.5625e-05,\n",
       " 6.25e-05,\n",
       " 0.00025,\n",
       " 0.001,\n",
       " 0.004,\n",
       " 0.016,\n",
       " 0.064,\n",
       " 0.256]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The search grid for C needs to be divided by the sample number\n",
    "[ele/X_train.shape[0] for ele in C_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbc214e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set labels distribution: {-1.0: 1958, 1.0: 2042}, pos:neg = 1.0429009193054137\n",
      "\n",
      "Validation set labels distribution: {-1.0: 2290, 1.0: 2209}, pos:neg = 0.9646288209606987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check whether the labels are balanced\n",
    "train_unique, train_counts = np.unique(Y_train, return_counts=True)\n",
    "val_unique, val_counts = np.unique(Y_val, return_counts=True)\n",
    "\n",
    "train_dict = dict(zip(train_unique, train_counts))\n",
    "val_dict = dict(zip(val_unique, val_counts))\n",
    "\n",
    "print(f'Training set labels distribution: {train_dict}, pos:neg = {train_dict[1.0]/train_dict[-1.0]}\\n')\n",
    "print(f'Validation set labels distribution: {val_dict}, pos:neg = {val_dict[1.0]/val_dict[-1.0]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea01429",
   "metadata": {},
   "source": [
    "This is not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d5f9d",
   "metadata": {},
   "source": [
    "### Grid Search find C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e7bad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "Best Para in Sklearn model: {'C': 0.004}, the corresponding C in last qustion: 16.0\n",
      "\n",
      "Test accuracy: 96.8%\n",
      "sum of W*:  -0.08312181246869857\n",
      "b*:         0.5394102610170345\n"
     ]
    }
   ],
   "source": [
    "# Using the grid search to find optimal C for SKlearn model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "C_grid_lst = [ele/X_train.shape[0] for ele in C_lst]\n",
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'C': C_grid_lst,\n",
    "}\n",
    "\n",
    "\n",
    "# Create a based model\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = 'accuracy',\n",
    "                          cv = 5, n_jobs = -1, verbose = 2) #to be consistent as last qustion, using accuracy as the metric\n",
    "grid_search.fit(X_val, Y_val)\n",
    "\n",
    "print(f'Best Para in Sklearn model: {grid_search.best_params_}, the corresponding C in last qustion: {list(grid_search.best_params_.values())[0]*X_train.shape[0]}\\n')\n",
    "\n",
    "#creat svm model using optimal paras\n",
    "svm_star = svm.LinearSVC(C=grid_search.best_params_['C'])\n",
    "#t fit train set\n",
    "svm_star.fit(X_train, Y_train)\n",
    "\n",
    "# Performance\n",
    "score = svm_star.score(X_test,Y_test)\n",
    "print(f'Test accuracy: {score*100}%')\n",
    "print('sum of W*: ',np.sum(svm_star.coef_))\n",
    "print('b*:        ',svm_star.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120b5db",
   "metadata": {},
   "source": [
    "The best parameter of C is slightly different from last question, because Q5 is trained on traing set and find the C that yields the highest accuracy on the validation set. This Q6 is using 5 folds cross validation to find the best C using the validation set only. Different dataset may lead to a slightly different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f3d11db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val + Train set labels distribution: {-1.0: 2290, 1.0: 2209}, pos:neg = 1.0007062146892656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Val + Train set labels distribution: {val_dict}, pos:neg = {(train_dict[1.0] + val_dict[1.0])/(train_dict[-1.0] + val_dict[-1.0])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec77413",
   "metadata": {},
   "source": [
    "In Q5, we used both of the training set and validation set, if we use both of them, as shown in the distribution result above, pos samples and the neg samples are well distributed compared to only using one set only. Therefore, using validation set only is slightly unbalanced compared to using both of train and val. To solve the unbalanced dataset problem, we need to change the scoring metric from 'accuracy' to 'roc_auc'. (Usually in a real-life project, we split the train,val,test in a balanced way in advance, which means in this assignment train an val both have well balanced distributions, but the assignment required to split it in first 4000 and rest 4499, so I can't change that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae0e45fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "Best Para in Sklearn model: {'C': 0.001}, the corresponding C in last qustion: 4.0\n",
      "\n",
      "Test accuracy: 96.66666666666667%\n",
      "sum of W*:  -0.038602420901886494\n",
      "b*:         0.29306626124222945\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, scoring = 'roc_auc',\n",
    "                          cv = 5, n_jobs = -1, verbose = 2) # Using roc_auc as the evaluation metric\n",
    "grid_search.fit(X_val, Y_val)\n",
    "\n",
    "print(f'Best Para in Sklearn model: {grid_search.best_params_}, the corresponding C in last qustion: {list(grid_search.best_params_.values())[0]*X_train.shape[0]}\\n')\n",
    "\n",
    "#create svm model using optimal paras\n",
    "svm_star = svm.LinearSVC(C=grid_search.best_params_['C'])\n",
    "#t fit train set\n",
    "svm_star.fit(X_train, Y_train)\n",
    "\n",
    "# Performance\n",
    "score = svm_star.score(X_test,Y_test)\n",
    "print(f'Test accuracy: {score*100}%')\n",
    "print('sum of W*: ',np.sum(svm_star.coef_))\n",
    "print('b*:        ',svm_star.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83ba2e",
   "metadata": {},
   "source": [
    "Using 'ROC_AUC' as the scoring metric is more recommanded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
